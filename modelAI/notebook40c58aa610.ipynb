{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:07.075147Z",
     "iopub.status.busy": "2022-08-14T19:22:07.074538Z",
     "iopub.status.idle": "2022-08-14T19:22:07.081014Z",
     "shell.execute_reply": "2022-08-14T19:22:07.080048Z",
     "shell.execute_reply.started": "2022-08-14T19:22:07.075115Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:07.892717Z",
     "iopub.status.busy": "2022-08-14T19:22:07.892328Z",
     "iopub.status.idle": "2022-08-14T19:22:07.898425Z",
     "shell.execute_reply": "2022-08-14T19:22:07.897456Z",
     "shell.execute_reply.started": "2022-08-14T19:22:07.892685Z"
    }
   },
   "outputs": [],
   "source": [
    "pathh = \"/home/demir/Desktop/BitirmeProjesi/getData/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:09.032789Z",
     "iopub.status.busy": "2022-08-14T19:22:09.031502Z",
     "iopub.status.idle": "2022-08-14T19:22:09.040933Z",
     "shell.execute_reply": "2022-08-14T19:22:09.039617Z",
     "shell.execute_reply.started": "2022-08-14T19:22:09.032741Z"
    }
   },
   "outputs": [],
   "source": [
    "class FolderDataset(Dataset):\n",
    "\n",
    "    def __init__(self, main_dir, transform=None):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs = sorted(os.listdir(pathh), key=self.extract_integer)\n",
    "        \n",
    "        \n",
    "        self.all_imgs.remove(\"622.png\")\n",
    "        self.all_imgs.remove(\"193.png\")\n",
    "        self.all_imgs.remove(\"18056.png\")\n",
    "        self.all_imgs.remove(\"5977.png\")\n",
    "        self.all_imgs.remove(\"16904.png\")\n",
    "        \n",
    "        self.all_imgs.remove(\"7209.png\")\n",
    "        self.all_imgs.remove(\"9094.png\")\n",
    "        self.all_imgs.remove(\"8494.png\")\n",
    "        self.all_imgs.remove(\"24245.png\")\n",
    "        \n",
    "    def extract_integer(self,filename):\n",
    "        return int(filename.split('.')[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        try:\n",
    "            img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
    "            image = Image.open(img_loc).convert(\"RGB\")\n",
    "            image = image.resize((512, 512))\n",
    "        except:\n",
    "            \n",
    "            print(self.main_dir)\n",
    "            print(self.all_imgs[idx])\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            tensor_image = self.transform(image)\n",
    "\n",
    "        return tensor_image, tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:10.227355Z",
     "iopub.status.busy": "2022-08-14T19:22:10.226409Z",
     "iopub.status.idle": "2022-08-14T19:22:10.258440Z",
     "shell.execute_reply": "2022-08-14T19:22:10.257271Z",
     "shell.execute_reply.started": "2022-08-14T19:22:10.227312Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Encoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.maxpool5 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(256, 512, (3, 3), padding=(1, 1))\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.maxpool6 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(512, 1024, (3, 3), padding=(1, 1))\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.maxpool7 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(1024, 2048, (3, 3), padding=(1, 1))\n",
    "        self.relu8 = nn.ReLU(inplace=True)\n",
    "        self.maxpool8 = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(2048, 2048, (3, 3), padding=(1, 1))\n",
    "        self.relu9 = nn.ReLU(inplace=True)\n",
    "        self.maxpool9 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.maxpool6(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.maxpool7(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = self.relu8(x)\n",
    "        x = self.maxpool8(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = self.relu9(x)\n",
    "        x = self.maxpool9(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:11.602890Z",
     "iopub.status.busy": "2022-08-14T19:22:11.601787Z",
     "iopub.status.idle": "2022-08-14T19:22:11.616597Z",
     "shell.execute_reply": "2022-08-14T19:22:11.615195Z",
     "shell.execute_reply.started": "2022-08-14T19:22:11.602847Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Decoder Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(2048, 2048, (2, 2), stride=(2, 2))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(2048, 1024, (2, 2), stride=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(1024, 512, (2, 2), stride=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(512, 256, (2, 2), stride=(2, 2))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv5 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv6 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv7 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv8 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))\n",
    "        self.relu8 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.deconv9 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))\n",
    "        self.relu9 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Upscale the image with convtranspose etc.\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        x = self.deconv6(x)\n",
    "        x = self.relu6(x)\n",
    "        \n",
    "        x = self.deconv7(x)\n",
    "        x = self.relu7(x)\n",
    "        \n",
    "        x = self.deconv8(x)\n",
    "        x = self.relu8(x)\n",
    "        \n",
    "        x = self.deconv9(x)\n",
    "        x = self.relu9(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T19:22:16.457540Z",
     "iopub.status.busy": "2022-08-14T19:22:16.457152Z",
     "iopub.status.idle": "2022-08-14T19:22:16.471854Z",
     "shell.execute_reply": "2022-08-14T19:22:16.470055Z",
     "shell.execute_reply.started": "2022-08-14T19:22:16.457496Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "    Performs a single training step\n",
    "    Args:\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder\n",
    "    train_loader: PyTorch dataloader, containing (images, images).\n",
    "    loss_fn: PyTorch loss_fn, computes loss between 2 images.\n",
    "    optimizer: PyTorch optimizer.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Train Loss\n",
    "    \"\"\"\n",
    "    #  Set networks to train mode.\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for batch_idx, (train_img, target_img) in enumerate(train_loader):\n",
    "        # Move images to device\n",
    "        train_img = train_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "        \n",
    "        # Zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Feed the train images to encoder\n",
    "        enc_output = encoder(train_img)\n",
    "        # The output of encoder is input to decoder !\n",
    "        dec_output = decoder(enc_output)\n",
    "        \n",
    "        # Decoder output is reconstructed image\n",
    "        # Compute loss with it and orginal image which is target image.\n",
    "        loss = loss_fn(dec_output, target_img)\n",
    "        # Backpropogate\n",
    "        loss.backward()\n",
    "        # Apply the optimizer to network by calling step.\n",
    "        optimizer.step()\n",
    "    # Return the loss\n",
    "    return loss.item()\n",
    "\n",
    "def val_step(encoder, decoder, val_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Performs a single training step\n",
    "    Args:\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder\n",
    "    val_loader: PyTorch dataloader, containing (images, images).\n",
    "    loss_fn: PyTorch loss_fn, computes loss between 2 images.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Validation Loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set to eval mode.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # We don't need to compute gradients while validating.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (train_img, target_img) in enumerate(val_loader):\n",
    "            # Move to device\n",
    "            train_img = train_img.to(device)\n",
    "            target_img = target_img.to(device)\n",
    "\n",
    "            # Again as train. Feed encoder the train image.\n",
    "            enc_output = encoder(train_img)\n",
    "            # Decoder takes encoder output and reconstructs the image.\n",
    "            dec_output = decoder(enc_output)\n",
    "\n",
    "            # Validation loss for encoder and decoder.\n",
    "            loss = loss_fn(dec_output, target_img)\n",
    "    # Return the loss\n",
    "    return loss.item()\n",
    "\n",
    "def create_embedding(encoder, full_loader, embedding_dim, device):\n",
    "    \"\"\"\n",
    "    Creates embedding using encoder from dataloader.\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    full_loader: PyTorch dataloader, containing (images, images) over entire dataset.\n",
    "    embedding_dim: Tuple (c, h, w) Dimension of embedding = output of encoder dimesntions.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Embedding of size (num_images_in_loader + 1, c, h, w)\n",
    "    \"\"\"\n",
    "    # Set encoder to eval mode.\n",
    "    encoder.eval()\n",
    "    # Just a place holder for our 0th image embedding.\n",
    "    embedding = torch.randn(embedding_dim)\n",
    "    \n",
    "    # Again we do not compute loss here so. No gradients.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (train_img, target_img) in enumerate(full_loader):\n",
    "            # We can compute this on GPU. be faster\n",
    "            train_img = train_img.to(device)\n",
    "            \n",
    "            # Get encoder outputs and move outputs to cpu\n",
    "            enc_output = encoder(train_img).cpu()\n",
    "            # Keep adding these outputs to embeddings.\n",
    "            embedding = torch.cat((embedding, enc_output), 0)\n",
    "    \n",
    "    # Return the embeddings\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T08:21:48.809662Z",
     "iopub.status.busy": "2022-08-14T08:21:48.808684Z",
     "iopub.status.idle": "2022-08-14T12:26:17.041689Z",
     "shell.execute_reply": "2022-08-14T12:26:17.040682Z",
     "shell.execute_reply.started": "2022-08-14T08:21:48.809578Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4783/1024795972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Shift models to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([T.ToTensor()]) # Normalize the pixels and convert to tensor.\n",
    "\n",
    "full_dataset = FolderDataset(pathh, transforms) # Create folder dataset.\n",
    "\n",
    "train_set_size = int(len(full_dataset) * 0.9)\n",
    "valid_set_size = len(full_dataset) - train_set_size\n",
    "\n",
    "\n",
    "# Split data to train and test\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_set_size, valid_set_size]) \n",
    "\n",
    "# Create the train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " \n",
    "# Create the validation dataloader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create the full dataloader\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32)\n",
    "\n",
    "loss_fn = nn.MSELoss() # We use Mean squared loss which computes difference between two images.\n",
    "\n",
    "encoder = ConvEncoder() # Our encoder model\n",
    "decoder = ConvDecoder() # Our decoder model\n",
    "\n",
    "device = \"cuda\"  # GPU device\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Both the enocder and decoder parameters\n",
    "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(autoencoder_params, lr=1e-3) # Adam Optimizer\n",
    "\n",
    "# Time to Train !!!\n",
    "EPOCHS = 50\n",
    "\n",
    "max_loss = 99\n",
    "\n",
    "# Usual Training Loop\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "        \n",
    "        val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)\n",
    "        \n",
    "        print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "        # Simple Best Model saving\n",
    "        if val_loss < max_loss:\n",
    "            print(\"Validation Loss decreased, saving new best model\")\n",
    "            torch.save(encoder.state_dict(), \"encoder_model.pt\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_model.pt\")\n",
    "            \n",
    "            max_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4995, 8632, 4999, 17097, 4835, 24092, 24088, 6928, 8630, 4315]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "device = \"cpu\"  # GPU device\n",
    "encoder = ConvEncoder()\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(\"/home/demir/Desktop/BitirmeProjesi/modelAI/encoder_model.pt\",map_location=torch.device('cpu')))\n",
    "encoder.eval()\n",
    "\n",
    "def compute_similar_images(image, num_images, embedding, device):\n",
    "    \"\"\"\n",
    "    Given an image and number of similar images to search.\n",
    "    Returns the num_images closest neares images.\n",
    "    Args:\n",
    "    image: Image whose similar images are to be found.\n",
    "    num_images: Number of similar images to find.\n",
    "    embedding : A (num_images, embedding_dim) Embedding of images learnt from auto-encoder.\n",
    "    device : \"cuda\" or \"cpu\" device.\n",
    "    \"\"\"\n",
    "    \n",
    "    image = Image.open(image).convert(\"RGB\")\n",
    "    image = image.resize((512, 512))\n",
    "    \n",
    "    image_tensor = T.ToTensor()(image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embedding = encoder(image_tensor).cpu().detach().numpy()\n",
    "        \n",
    "    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(embedding)\n",
    "\n",
    "    a, indices = knn.kneighbors(flattened_embedding)\n",
    "    indices_list = indices.tolist()\n",
    "    return indices_list\n",
    "\n",
    "device=\"cpu\"\n",
    "flattened_embedding = np.load(\"/home/demir/Desktop/BitirmeProjesi/modelAI/data_embedding.npy\")\n",
    "\n",
    "\n",
    "imgList = compute_similar_images(\"/home/demir/Desktop/BitirmeProjesi/getData/data/38250.png\",10,flattened_embedding, device)\n",
    "\n",
    "imgList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5003.png'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_imgs[4995-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvEncoder(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (maxpool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4): ReLU(inplace=True)\n",
       "  (maxpool4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu5): ReLU(inplace=True)\n",
       "  (maxpool5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu6): ReLU(inplace=True)\n",
       "  (maxpool6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv7): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu7): ReLU(inplace=True)\n",
       "  (maxpool7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv8): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu8): ReLU(inplace=True)\n",
       "  (maxpool8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv9): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu9): ReLU(inplace=True)\n",
       "  (maxpool9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = \"cpu\"  # GPU device\n",
    "\n",
    "encoder = ConvEncoder()\n",
    "encoder.to(device)\n",
    "encoder.load_state_dict(torch.load(\"/home/demir/Desktop/BitirmeProjesi/modelAI/encoder_model.pt\",map_location=torch.device('cpu') ))\n",
    "encoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([T.ToTensor()]) # Normalize the pixels and convert to tensor.\n",
    "\n",
    "full_dataset = FolderDataset(pathh, transforms) # Create folder dataset.\n",
    "\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "encoder = ConvEncoder() # Our encoder model\n",
    "\n",
    "\n",
    "device = \"cpu\"  # GPU device\n",
    "\n",
    "# Shift models to GPU\n",
    "encoder.to(device)\n",
    "\n",
    "# Save the feature representations.\n",
    "EMBEDDING_SHAPE = (1, 2048, 1, 1) # This we know from our encoder\n",
    "\n",
    "# We need feature representations for complete dataset not just train and validation.\n",
    "# Hence we use full loader here.\n",
    "embedding = create_embedding(encoder, full_loader, EMBEDDING_SHAPE, device)\n",
    "\n",
    "# Convert embedding to numpy and save them\n",
    "numpy_embedding = embedding.cpu().detach().numpy()\n",
    "num_images = numpy_embedding.shape[0]\n",
    "\n",
    "# Save the embeddings for complete dataset, not just train\n",
    "flattened_embedding = numpy_embedding.reshape((num_images, -1))\n",
    "np.save(\"data_embedding.npy\", flattened_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
